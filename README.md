This project shows:
- **spaCy Tokenizer** for word-level tokenization
- **BPE (Byte Pair Encoding)** for subword tokenization
- **Word_pair** of both outputs

---

# Tokenizer Example
### 1Ô∏è‚É£ Install & Download Model

pip install spacy
python -m spacy download xx_ent_wiki_sm

input : ‡∞∞‡∞ø‡∞Ø‡∞≤‡±ç ‡∞é‡∞∏‡±ç‡∞ü‡±á‡∞ü‡±ç: ‡∞è‡∞ê ‡∞µ‡∞æ‡∞°‡∞ï‡∞Ç ‡∞µ‡∞≤‡±ç‡∞≤ ‡∞∞‡∞ø‡∞Ø‡∞≤‡±ç ‡∞é‡∞∏‡±ç‡∞ü‡±á‡∞ü‡±ç ‡∞∞‡∞Ç‡∞ó‡∞Ç‡∞≤‡±ã ‡∞®‡∞ø‡∞∞‡±ç‡∞Æ‡∞æ‡∞£ ‡∞ñ‡∞∞‡±ç‡∞ö‡±Å‡∞≤‡±Å ‡∞§‡∞ó‡±ç‡∞ó‡±Å‡∞§‡∞æ‡∞Ø‡∞®‡∞ø,‡∞™‡±ç‡∞∞‡∞æ‡∞ú‡±Ü‡∞ï‡±ç‡∞ü‡±ç ‡∞∏‡∞Æ‡∞Ø‡∞Ç ‡∞∏‡∞ó‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞§‡∞ó‡±ç‡∞ó‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞®‡∞ø ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞ú‡∞æ‡∞™‡±ç‡∞Ø‡∞æ‡∞≤‡±Å ‡∞§‡∞ó‡±ç‡∞ó‡±Å‡∞§‡∞æ‡∞Ø‡∞®‡∞ø ‡∞í‡∞ï ‡∞µ‡∞æ‡∞∞‡±ç‡∞§ ‡∞®‡∞ø‡∞µ‡±á‡∞¶‡∞ø‡∞Ç‡∞ö‡∞ø‡∞Ç‡∞¶‡∞ø.‡∞á‡∞¶‡∞ø ‡∞ó‡±É‡∞π ‡∞ï‡±ä‡∞®‡±Å‡∞ó‡±ã‡∞≤‡±Å‡∞¶‡∞æ‡∞∞‡±Å‡∞≤‡∞ï‡±Å ‡∞™‡±ç‡∞∞‡∞Ø‡±ã‡∞ú‡∞®‡∞Ç ‡∞ö‡±á‡∞ï‡±Ç‡∞∞‡±Å‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.

### üßÆ Example Run

```python
import spacy

# Load the multi-language spaCy model
nlp = spacy.load("xx_ent_wiki_sm")

# Telugu input text
text = "‡∞∞‡∞ø‡∞Ø‡∞≤‡±ç ‡∞é‡∞∏‡±ç‡∞ü‡±á‡∞ü‡±ç: ‡∞è‡∞ê ‡∞µ‡∞æ‡∞°‡∞ï‡∞Ç ‡∞µ‡∞≤‡±ç‡∞≤ ‡∞∞‡∞ø‡∞Ø‡∞≤‡±ç ‡∞é‡∞∏‡±ç‡∞ü‡±á‡∞ü‡±ç ‡∞∞‡∞Ç‡∞ó‡∞Ç‡∞≤‡±ã ‡∞®‡∞ø‡∞∞‡±ç‡∞Æ‡∞æ‡∞£ ‡∞ñ‡∞∞‡±ç‡∞ö‡±Å‡∞≤‡±Å ‡∞§‡∞ó‡±ç‡∞ó‡±Å‡∞§‡∞æ‡∞Ø‡∞®‡∞ø, ‡∞™‡±ç‡∞∞‡∞æ‡∞ú‡±Ü‡∞ï‡±ç‡∞ü‡±ç ‡∞∏‡∞Æ‡∞Ø‡∞Ç ‡∞∏‡∞ó‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞§‡∞ó‡±ç‡∞ó‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞®‡∞ø ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞ú‡∞æ‡∞™‡±ç‡∞Ø‡∞æ‡∞≤‡±Å ‡∞§‡∞ó‡±ç‡∞ó‡±Å‡∞§‡∞æ‡∞Ø‡∞®‡∞ø ‡∞í‡∞ï ‡∞µ‡∞æ‡∞∞‡±ç‡∞§ ‡∞®‡∞ø‡∞µ‡±á‡∞¶‡∞ø‡∞Ç‡∞ö‡∞ø‡∞Ç‡∞¶‡∞ø."

# Manual tokenization (splitting by spaces)
manual_tokens = text.split()

# spaCy tokenization
doc = nlp(text)
spacy_tokens = [token.text for token in doc]

# Compare results
print(f"Manual token count: {len(manual_tokens)}")
print(f"spaCy token count: {len(spacy_tokens)}")
print(f"Tokens that differ: {len(set(manual_tokens) ^ set(spacy_tokens))}")
```

# BPE Learner
### üßÆ Example Run
```python
input : low low low low low lowest lowest newer newer newer newer newer newer wider wider wider new new

Enter number of merges to learn (default 10): 3
```

# BPE paragraph learner 
### üßÆ Example Run
```python

input : The quick brown fox jumps over the lazy dog. Natural language processing is amazing with subword tokenization.This demonstrates BPE algorithm effectiveness for various NLP tasks.

Enter number of merges to learn (default 30): 25

Enter : 
tokenization
jumps
processing
effectiveness
algorithm
```

# Word pair

### üßÆ Example Run
```python
source = "Sunday"
target = "Saturday"

# Model A: Equal cost for all operations
distA = min_edit_distance(source, target, 1, 1, 1)
print(f"Model A distance: {distA}")

# Model B: Higher cost for substitution
distB = min_edit_distance(source, target, 2, 1, 1)
print(f"Model B distance: {distB}")
```